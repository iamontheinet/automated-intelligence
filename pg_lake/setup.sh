#!/bin/bash
set -e

cd "$(dirname "$0")"

echo "=== pg_lake Demo Setup ==="
echo ""

# Check prerequisites
if ! command -v docker &> /dev/null; then
    echo "❌ Docker not found. Please install Docker first."
    exit 1
fi

if ! command -v docker compose &> /dev/null && ! docker compose version &> /dev/null; then
    echo "❌ Docker Compose not found. Please install Docker Compose."
    exit 1
fi

if [ ! -f ~/.aws/credentials ]; then
    echo "⚠️  AWS credentials not found at ~/.aws/credentials"
    echo "   pg_lake needs AWS credentials for S3 access."
    echo ""
fi

# Snowflake connection name (override with SNOWFLAKE_CONNECTION_NAME env var)
SF_CONNECTION="${SNOWFLAKE_CONNECTION_NAME:-dash-builder-si}"

echo "Fetching latest Iceberg metadata paths from Snowflake..."
echo "  Using connection: $SF_CONNECTION"
echo ""

QUERY="SELECT 
    'PRODUCT_REVIEWS' as table_name,
    PARSE_JSON(SYSTEM\$GET_ICEBERG_TABLE_INFORMATION('AUTOMATED_INTELLIGENCE.PG_LAKE.PRODUCT_REVIEWS')):metadataLocation::STRING as metadata_location
UNION ALL
SELECT 
    'SUPPORT_TICKETS',
    PARSE_JSON(SYSTEM\$GET_ICEBERG_TABLE_INFORMATION('AUTOMATED_INTELLIGENCE.PG_LAKE.SUPPORT_TICKETS')):metadataLocation::STRING"

# Query Snowflake for current metadata locations using snow CLI
METADATA_JSON=$(snow sql -c "$SF_CONNECTION" --format json -q "$QUERY")
PRODUCT_REVIEWS_PATH=$(echo "$METADATA_JSON" | python3 -c "import sys,json; data=json.load(sys.stdin); print([r['METADATA_LOCATION'] for r in data if r['TABLE_NAME']=='PRODUCT_REVIEWS'][0])")
SUPPORT_TICKETS_PATH=$(echo "$METADATA_JSON" | python3 -c "import sys,json; data=json.load(sys.stdin); print([r['METADATA_LOCATION'] for r in data if r['TABLE_NAME']=='SUPPORT_TICKETS'][0])")

if [ -z "$PRODUCT_REVIEWS_PATH" ] || [ -z "$SUPPORT_TICKETS_PATH" ]; then
    echo "❌ Failed to get metadata paths from Snowflake."
    echo "   Make sure the Iceberg tables exist in AUTOMATED_INTELLIGENCE.PG_LAKE"
    echo "   Run snowflake_export.sql first if needed."
    exit 1
fi

echo "✓ Product Reviews: $PRODUCT_REVIEWS_PATH"
echo "✓ Support Tickets: $SUPPORT_TICKETS_PATH"
echo ""

# Generate init-postgres.sql with current paths
echo "Generating init-postgres.sql with latest paths..."
cat > scripts/init-postgres.sql << EOF
create extension if not exists pg_lake_spatial cascade;

-- Foreign tables for Snowflake Iceberg data
-- These point to Iceberg metadata files (NOT raw parquet) to preserve Iceberg semantics
-- Auto-generated by setup.sh at $(date)

DROP FOREIGN TABLE IF EXISTS product_reviews;
CREATE FOREIGN TABLE product_reviews()
SERVER pg_lake
OPTIONS (path '$PRODUCT_REVIEWS_PATH');

DROP FOREIGN TABLE IF EXISTS support_tickets;
CREATE FOREIGN TABLE support_tickets()
SERVER pg_lake
OPTIONS (path '$SUPPORT_TICKETS_PATH');
EOF

echo "✓ Generated scripts/init-postgres.sql"
echo ""

echo "Starting pg_lake container..."
docker compose down -v 2>/dev/null || true
docker compose up -d

echo ""
echo "Waiting for pg_lake to be ready..."
sleep 10

# Check if container is healthy
RETRIES=12
until docker compose exec -T pg_lake pg_isready -U postgres > /dev/null 2>&1 || [ $RETRIES -eq 0 ]; do
    echo "  Waiting for PostgreSQL... ($RETRIES attempts left)"
    RETRIES=$((RETRIES-1))
    sleep 5
done

if [ $RETRIES -eq 0 ]; then
    echo "❌ pg_lake failed to start. Check logs:"
    echo "   docker compose logs pg_lake"
    exit 1
fi

echo ""
echo "✅ pg_lake is ready!"
echo ""
echo "=== Connection Info ==="
echo "  Host:     localhost"
echo "  Port:     5433"
echo "  User:     postgres"
echo "  Password: postgres"
echo "  Database: postgres"
echo ""
echo "=== Quick Commands ==="
echo "  Connect:  psql -h localhost -p 5433 -U postgres -d postgres"
echo "  Demo:     PGPASSWORD=postgres psql -h localhost -p 5433 -U postgres -d postgres --pset pager=off -f demo_queries.sql"
echo "  Logs:     docker compose logs -f pg_lake"
echo "  Stop:     docker compose down"
echo ""
